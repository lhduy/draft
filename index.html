<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <title>A Pelican Blog</title>
        <link rel="stylesheet" href="/theme/css/main.css" />
        <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="A Pelican Blog Atom Feed" />

        <!--[if IE]>
            <script src="https://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
        <![endif]-->
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="/">A Pelican Blog </a></h1>
                <nav><ul>
                    <li><a href="/category/machine-learning.html">machine learning</a></li>
                </ul></nav>
        </header><!-- /#banner -->

            <aside id="featured" class="body">
                <article>
                    <h1 class="entry-title"><a href="/machine-learning-in-fraud-analysis.html">Machine Learning in Fraud Analysis</a></h1>
<footer class="post-info">
        <abbr class="published" title="2017-05-15T00:00:00+00:00">
                Published: Mon 15 May 2017
        </abbr>

<p>In <a href="/category/machine-learning.html">machine learning</a>.</p>

</footer><!-- /.post-info --><h1>Enron Fraud Indentification</h1>
<p>The main purpose of this project is using machine learning algorithm to detect fraudsters. Those persons are main criminals in Enron scandal. We would like to make a learning model to predict those people by using multiple variables which are reported in financial sheets and emailling list. Because we have already know person of interests (POI), we could use supervised machine learning to do this task. </p>
<div class="highlight"><pre><span></span><span class="ch">#!/usr/bin/python</span>

<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">pickle</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;../tools/&quot;</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">tester</span>

<span class="kn">import</span> <span class="nn">tester</span>
<span class="kn">from</span> <span class="nn">feature_format</span> <span class="kn">import</span> <span class="n">featureFormat</span><span class="p">,</span> <span class="n">targetFeatureSplit</span>
<span class="kn">from</span> <span class="nn">tester</span> <span class="kn">import</span> <span class="n">dump_classifier_and_data</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">time</span>
<span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span>
<span class="kn">from</span> <span class="nn">tabulate</span> <span class="kn">import</span> <span class="n">tabulate</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="kn">as</span> <span class="nn">sns</span>

<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MinMaxScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SelectKBest</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">chi2</span>

<span class="kn">from</span> <span class="nn">sklearn.grid_search</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">StratifiedShuffleSplit</span>

<span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">AdaBoostClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingClassifier</span>  <span class="c1">#GBM algorithm</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">xgboost.sklearn</span> <span class="kn">import</span> <span class="n">XGBClassifier</span>
<span class="kn">import</span> <span class="nn">xgboost</span> <span class="kn">as</span> <span class="nn">xgb</span>

<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span> 
</pre></div>


<h2>1. Dataset Exploration</h2>
<div class="highlight"><pre><span></span><span class="c1">### Load the dictionary containing the dataset</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;final_project_dataset.pkl&quot;</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">data_file</span><span class="p">:</span>
    <span class="n">data_dict</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">data_file</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="c1">### features_list is a list of strings, each of which is a feature name.</span>
<span class="c1">### The first feature must be &quot;poi&quot;.</span>
<span class="n">target_list</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;poi&#39;</span><span class="p">]</span>
<span class="n">report_list</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;salary&#39;</span><span class="p">,</span><span class="s1">&#39;deferral_payments&#39;</span><span class="p">,</span> <span class="s1">&#39;total_payments&#39;</span><span class="p">,</span>
               <span class="s1">&#39;loan_advances&#39;</span><span class="p">,</span> <span class="s1">&#39;bonus&#39;</span><span class="p">,</span> <span class="s1">&#39;restricted_stock_deferred&#39;</span><span class="p">,</span> 
               <span class="s1">&#39;deferred_income&#39;</span><span class="p">,</span> <span class="s1">&#39;total_stock_value&#39;</span><span class="p">,</span> <span class="s1">&#39;expenses&#39;</span><span class="p">,</span>
               <span class="s1">&#39;exercised_stock_options&#39;</span><span class="p">,</span> <span class="s1">&#39;other&#39;</span><span class="p">,</span> <span class="s1">&#39;long_term_incentive&#39;</span><span class="p">,</span> 
               <span class="s1">&#39;restricted_stock&#39;</span><span class="p">,</span> <span class="s1">&#39;director_fees&#39;</span><span class="p">,</span><span class="s1">&#39;to_messages&#39;</span><span class="p">,</span>
               <span class="s1">&#39;from_poi_to_this_person&#39;</span><span class="p">,</span> <span class="s1">&#39;from_messages&#39;</span><span class="p">,</span> <span class="s1">&#39;from_this_person_to_poi&#39;</span><span class="p">,</span> 
               <span class="s1">&#39;shared_receipt_with_poi&#39;</span><span class="p">]</span>
<span class="n">features_list</span> <span class="o">=</span>  <span class="n">target_list</span> <span class="o">+</span> <span class="n">report_list</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="c1">### Store to my_dataset for easy export below.</span>
<span class="n">my_dataset</span> <span class="o">=</span> <span class="n">data_dict</span>
<span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">entry</span> <span class="ow">in</span> <span class="n">my_dataset</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">my_dataset</span><span class="p">[</span><span class="n">entry</span><span class="p">][</span><span class="s1">&#39;poi&#39;</span><span class="p">]:</span>
        <span class="n">counter</span> <span class="o">+=</span> <span class="mi">1</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="k">print</span> <span class="s1">&#39;Number of data points in this dataset: </span><span class="si">%f</span><span class="s1">&#39;</span> <span class="o">%</span><span class="nb">len</span><span class="p">(</span><span class="n">my_dataset</span><span class="p">)</span>
<span class="k">print</span> <span class="s1">&#39;Number of POI in this dataset: </span><span class="si">%f</span><span class="s1">&#39;</span> <span class="o">%</span><span class="n">counter</span>
<span class="k">print</span> <span class="s1">&#39;Number of feature in this dataset: </span><span class="si">%f</span><span class="s1">&#39;</span> <span class="o">%</span><span class="nb">len</span><span class="p">(</span><span class="n">features_list</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>Number of data points in this dataset: 146.000000
Number of POI in this dataset: 18.000000
Number of feature in this dataset: 20.000000
</pre></div>


<p>There are total 18 POIs in total 146 data points. Next step will be the data auditing to determine missing values or outliers.</p>
<h3>Data audit</h3>
<div class="highlight"><pre><span></span><span class="c1">### Determine missing values in Enron report</span>
<span class="n">value_report_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
<span class="n">dict_report_list</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">report_list</span><span class="p">,</span><span class="n">value_report_list</span><span class="p">))</span>
<span class="k">for</span> <span class="n">entry</span> <span class="ow">in</span> <span class="n">my_dataset</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">report_list</span><span class="p">:</span>
        <span class="n">val</span> <span class="o">=</span> <span class="n">my_dataset</span><span class="p">[</span><span class="n">entry</span><span class="p">][</span><span class="n">name</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">val</span> <span class="o">==</span> <span class="s1">&#39;NaN&#39;</span><span class="p">:</span>
            <span class="n">dict_report_list</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="n">NaN_table</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dict_report_list</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Feature&#39;</span><span class="p">,</span> <span class="s1">&#39;No of Missing Values&#39;</span><span class="p">])</span>
<span class="k">print</span> <span class="n">NaN_table</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">columns</span> <span class="o">=</span> <span class="s1">&#39;No of Missing Values&#39;</span><span class="p">,</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>                      Feature  No of Missing Values
11              loan_advances                   142
6               director_fees                   129
7   restricted_stock_deferred                   128
2           deferral_payments                   107
15            deferred_income                    97
18        long_term_incentive                    80
5                       bonus                    64
16    shared_receipt_with_poi                    60
14    from_this_person_to_poi                    60
1                 to_messages                    60
10    from_poi_to_this_person                    60
12              from_messages                    60
13                      other                    53
0                      salary                    51
9                    expenses                    51
4     exercised_stock_options                    44
17           restricted_stock                    36
3              total_payments                    21
8           total_stock_value                    20


C:\Anaconda2\lib\site-packages\ipykernel\__main__.py:10: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)
</pre></div>


<p>Through above table, every features except 'poi' has missing values at least 20 times. Top 3 features that have highest missing values are 'loan_advance', 'director_fees' and 'restricted_stock_deferred'. </p>
<p>According to Enron printed sheet, I found two outlier that is "TOTAL" and "THE TRAVEL AGENCY IN THE PARK". 
- "TOTAL" is the summary of every column in Enron sheet and it is a spreadsheet quirk.
- "THE TRAVEL AGENCY IN THE PARK" is the account of business-related travel to The Travel Agency in the Park. It is not a person of interest.</p>
<p>Another possibility is the observation with no information (all "NaN" value) which will determined by the script beblow.</p>
<div class="highlight"><pre><span></span><span class="n">outlier_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="c1">#### Next script is used to remove observation which have all value &#39;NaN&#39; in Enron report. </span>
<span class="k">for</span> <span class="n">entry</span> <span class="ow">in</span> <span class="n">my_dataset</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">report_list</span><span class="p">:</span>
        <span class="n">val</span> <span class="o">=</span> <span class="n">my_dataset</span><span class="p">[</span><span class="n">entry</span><span class="p">][</span><span class="n">name</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">val</span> <span class="o">==</span> <span class="s1">&#39;NaN&#39;</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">entry</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">outlier_list</span><span class="p">:</span>
                <span class="n">outlier_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">entry</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">val</span> <span class="o">!=</span> <span class="s1">&#39;NaN&#39;</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">outlier_list</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">entry</span><span class="p">)</span>
                <span class="k">break</span>
            <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
                <span class="k">break</span>

<span class="n">outlier_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;TOTAL&#39;</span><span class="p">)</span> <span class="c1">#insert &#39;TOTAL&#39; observation</span>
<span class="n">outlier_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;THE TRAVEL AGENCY IN THE PARK&#39;</span><span class="p">)</span>
<span class="k">print</span> <span class="s1">&#39;The outlier list is: &#39;</span>
<span class="k">print</span> <span class="n">outlier_list</span>
</pre></div>


<div class="highlight"><pre><span></span>The outlier list is: 
[&#39;LOCKHART EUGENE E&#39;, &#39;TOTAL&#39;, &#39;THE TRAVEL AGENCY IN THE PARK&#39;]
</pre></div>


<h3>Remove outlier</h3>
<div class="highlight"><pre><span></span><span class="c1">#### Remove outlier</span>
<span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">outlier_list</span><span class="p">:</span>
    <span class="n">my_dataset</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">name</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="k">print</span> <span class="s1">&#39;Number of data points after outlier removing in this dataset: </span><span class="si">%f</span><span class="s1">&#39;</span> <span class="o">%</span><span class="nb">len</span><span class="p">(</span><span class="n">my_dataset</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>Number of data points after outlier removing in this dataset: 143.000000
</pre></div>


<h2>2. Feature Selection</h2>
<h3>Create new feature</h3>
<p>Through the Enron report, we have the email list of Enron POI and non-POI. It will be smart to create a new feature based on the ratio between to/from POI and to/from a person. There is a chance that they can be used in POI prediction later.</p>
<div class="highlight"><pre><span></span><span class="c1">#### Making new feature based on other features.</span>
<span class="k">def</span> <span class="nf">computeFraction</span><span class="p">(</span> <span class="n">poi_messages</span><span class="p">,</span> <span class="n">all_messages</span> <span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; given a number messages to/from POI (numerator) </span>
<span class="sd">        and number of all messages to/from a person (denominator),</span>
<span class="sd">        return the fraction of messages to/from that person</span>
<span class="sd">        that are from/to a POI</span>
<span class="sd">   &quot;&quot;&quot;</span>

    <span class="c1">### you fill in this code, so that it returns either</span>
    <span class="c1">###     the fraction of all messages to this person that come from POIs</span>
    <span class="c1">###     or</span>
    <span class="c1">###     the fraction of all messages from this person that are sent to POIs</span>
    <span class="c1">### the same code can be used to compute either quantity</span>

    <span class="c1">### beware of &quot;NaN&quot; when there is no known email address (and so</span>
    <span class="c1">### no filled email features), and integer division!</span>
    <span class="c1">### in case of poi_messages or all_messages having &quot;NaN&quot; value, return 0.</span>
    <span class="k">if</span> <span class="n">poi_messages</span> <span class="o">==</span> <span class="s2">&quot;NaN&quot;</span><span class="p">:</span>
        <span class="n">fraction</span> <span class="o">=</span> <span class="mf">0.</span>
    <span class="k">elif</span> <span class="n">all_messages</span> <span class="o">==</span> <span class="s2">&quot;NaN&quot;</span><span class="p">:</span>
        <span class="n">fraction</span> <span class="o">=</span> <span class="mf">0.</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">fraction</span> <span class="o">=</span> <span class="n">poi_messages</span><span class="o">*</span><span class="mf">1.0</span><span class="o">/</span><span class="n">all_messages</span>

    <span class="k">return</span> <span class="n">fraction</span>

<span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">my_dataset</span><span class="p">:</span>
    <span class="n">data_point</span> <span class="o">=</span> <span class="n">my_dataset</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
    <span class="n">from_poi_to_this_person</span> <span class="o">=</span> <span class="n">data_point</span><span class="p">[</span><span class="s2">&quot;from_poi_to_this_person&quot;</span><span class="p">]</span>
    <span class="n">to_messages</span> <span class="o">=</span> <span class="n">data_point</span><span class="p">[</span><span class="s2">&quot;to_messages&quot;</span><span class="p">]</span>
    <span class="n">fraction_from_poi</span> <span class="o">=</span> <span class="n">computeFraction</span><span class="p">(</span> <span class="n">from_poi_to_this_person</span><span class="p">,</span> <span class="n">to_messages</span> <span class="p">)</span>
    <span class="n">my_dataset</span><span class="p">[</span><span class="n">name</span><span class="p">][</span><span class="s2">&quot;fraction_from_poi&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">fraction_from_poi</span>

    <span class="n">from_this_person_to_poi</span> <span class="o">=</span> <span class="n">data_point</span><span class="p">[</span><span class="s2">&quot;from_this_person_to_poi&quot;</span><span class="p">]</span>
    <span class="n">from_messages</span> <span class="o">=</span> <span class="n">data_point</span><span class="p">[</span><span class="s2">&quot;from_messages&quot;</span><span class="p">]</span>
    <span class="n">fraction_to_poi</span> <span class="o">=</span> <span class="n">computeFraction</span><span class="p">(</span> <span class="n">from_this_person_to_poi</span><span class="p">,</span> <span class="n">from_messages</span> <span class="p">)</span>
    <span class="n">my_dataset</span><span class="p">[</span><span class="n">name</span><span class="p">][</span><span class="s2">&quot;fraction_to_poi&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">fraction_to_poi</span>

<span class="c1">### Feature list with addtional features</span>
<span class="n">features_list</span> <span class="o">=</span> <span class="n">features_list</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;fraction_from_poi&#39;</span><span class="p">,</span><span class="s1">&#39;fraction_to_poi&#39;</span><span class="p">]</span>
<span class="k">print</span> <span class="n">features_list</span>
</pre></div>


<div class="highlight"><pre><span></span>[&#39;poi&#39;, &#39;salary&#39;, &#39;deferral_payments&#39;, &#39;total_payments&#39;, &#39;loan_advances&#39;, &#39;bonus&#39;, &#39;restricted_stock_deferred&#39;, &#39;deferred_income&#39;, &#39;total_stock_value&#39;, &#39;expenses&#39;, &#39;exercised_stock_options&#39;, &#39;other&#39;, &#39;long_term_incentive&#39;, &#39;restricted_stock&#39;, &#39;director_fees&#39;, &#39;to_messages&#39;, &#39;from_poi_to_this_person&#39;, &#39;from_messages&#39;, &#39;from_this_person_to_poi&#39;, &#39;shared_receipt_with_poi&#39;, &#39;fraction_from_poi&#39;, &#39;fraction_to_poi&#39;]
</pre></div>


<h3>Feature selection</h3>
<p>In order to determine the best features for prediction, I have used 'SelectKBest' function to evaluate the importance of each features. The function was applied in training set created by 'train_test_split' function.</p>
<div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">featureFormat</span><span class="p">(</span><span class="n">my_dataset</span><span class="p">,</span> <span class="n">features_list</span><span class="p">,</span> <span class="n">sort_keys</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">labels</span><span class="p">,</span> <span class="n">features</span> <span class="o">=</span> <span class="n">targetFeatureSplit</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="c1">### Using SelectKBest to determine number of selection feature.</span>
<span class="n">SKB</span> <span class="o">=</span> <span class="n">SelectKBest</span><span class="p">()</span>
<span class="n">SKB</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="n">labels</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">SKB</span><span class="o">.</span><span class="n">scores_</span>
<span class="n">unsorted_pairs</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="n">features_list</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">scores</span><span class="p">)</span>
<span class="n">sorted_pairs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="n">unsorted_pairs</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])))</span>
<span class="n">KBest_table</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">sorted_pairs</span><span class="p">,</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Feature&#39;</span><span class="p">,</span><span class="s1">&#39;Score&#39;</span><span class="p">])</span>
<span class="k">print</span> <span class="n">KBest_table</span>
</pre></div>


<div class="highlight"><pre><span></span>                      Feature      Score
0     exercised_stock_options  24.815080
1           total_stock_value  24.182899
2                       bonus  20.792252
3                      salary  18.289684
4             fraction_to_poi  16.409713
5             deferred_income  11.458477
6         long_term_incentive   9.922186
7            restricted_stock   9.212811
8              total_payments   8.772778
9     shared_receipt_with_poi   8.589421
10              loan_advances   7.184056
11                   expenses   6.094173
12    from_poi_to_this_person   5.243450
13                      other   4.187478
14          fraction_from_poi   3.128092
15    from_this_person_to_poi   2.382612
16              director_fees   2.126328
17                to_messages   1.646341
18          deferral_payments   0.224611
19              from_messages   0.169701
20  restricted_stock_deferred   0.065500
</pre></div>


<div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">KBest_table</span><span class="p">[</span><span class="s1">&#39;Feature&#39;</span><span class="p">],</span><span class="n">KBest_table</span><span class="p">[</span><span class="s1">&#39;Score&#39;</span><span class="p">])</span>
<span class="n">_</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">setp</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;SKBest score&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Classifier&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img alt="png" src="images/Enron_ML_24_0.png"></p>
<p>According to the result of SelectKBest, I selected 6 features of interest since they have highest impact for the model fitting (score &gt; 10). Clearly, we see that our new feature "fraction_to_poi" has high impact on the data model when it is at top 5th. Would this enginereed variable affect the validation of POIs in classifer ? To answer this question, I made an classifer testing with two feature lists: original list (without 'fraction_to_poi') and engineered list (with 'fraction_to_poi').</p>
<div class="highlight"><pre><span></span><span class="c1">### Validate the original feature list in Naive Bayes classifier</span>
<span class="n">old_features_list</span> <span class="o">=</span> <span class="n">target_list</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">KBest_table</span><span class="p">[</span><span class="s1">&#39;Feature&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="mi">6</span><span class="p">])</span>
<span class="n">old_features_list</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="s1">&#39;fraction_to_poi&#39;</span><span class="p">)</span>
<span class="k">print</span> <span class="n">old_features_list</span>
<span class="c1"># Re-create the data for model fitting</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">featureFormat</span><span class="p">(</span><span class="n">my_dataset</span><span class="p">,</span> <span class="n">old_features_list</span><span class="p">,</span> <span class="n">sort_keys</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">labels</span><span class="p">,</span> <span class="n">features</span> <span class="o">=</span> <span class="n">targetFeatureSplit</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="c1"># Example starting point. Create train and test set from original dataset. 30% of data goes into test set.</span>
<span class="n">features_train</span><span class="p">,</span> <span class="n">features_test</span><span class="p">,</span> <span class="n">labels_train</span><span class="p">,</span> <span class="n">labels_test</span> <span class="o">=</span> \
    <span class="n">train_test_split</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">clf_NB</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
<span class="n">clf_NB</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">features_train</span><span class="p">,</span><span class="n">labels_train</span><span class="p">)</span>
<span class="n">labels_pred</span> <span class="o">=</span> <span class="n">clf_NB</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">features_test</span><span class="p">)</span>
<span class="k">print</span> <span class="n">classification_report</span><span class="p">(</span><span class="n">labels_test</span><span class="p">,</span> <span class="n">labels_pred</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="k">[&#39;poi&#39;, &#39;exercised_stock_options&#39;, &#39;total_stock_value&#39;, &#39;bonus&#39;, &#39;salary&#39;, &#39;deferred_income&#39;]</span>
             <span class="err">precision</span>    <span class="err">recall</span>  <span class="err">f1-score</span>   <span class="err">support</span>

        <span class="err">0.0</span>       <span class="err">0.95</span>      <span class="err">0.95</span>      <span class="err">0.95</span>        <span class="err">39</span>
        <span class="err">1.0</span>       <span class="err">0.33</span>      <span class="err">0.33</span>      <span class="err">0.33</span>         <span class="err">3</span>

<span class="err">avg</span> <span class="err">/</span> <span class="err">total</span>       <span class="err">0.90</span>      <span class="err">0.90</span>      <span class="err">0.90</span>        <span class="err">42</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="c1">### Validate the engineered feature list in Naive Bayes classifier</span>
<span class="n">new_features_list</span> <span class="o">=</span> <span class="n">target_list</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">KBest_table</span><span class="p">[</span><span class="s1">&#39;Feature&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="mi">6</span><span class="p">])</span>
<span class="k">print</span> <span class="n">new_features_list</span>
<span class="c1"># Re-create the data for model fitting</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">featureFormat</span><span class="p">(</span><span class="n">my_dataset</span><span class="p">,</span> <span class="n">new_features_list</span><span class="p">,</span> <span class="n">sort_keys</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">labels</span><span class="p">,</span> <span class="n">features</span> <span class="o">=</span> <span class="n">targetFeatureSplit</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="c1"># Example starting point. Create train and test set from original dataset. 30% of data goes into test set.</span>
<span class="n">features_train</span><span class="p">,</span> <span class="n">features_test</span><span class="p">,</span> <span class="n">labels_train</span><span class="p">,</span> <span class="n">labels_test</span> <span class="o">=</span> \
    <span class="n">train_test_split</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">clf_NB</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
<span class="n">clf_NB</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">features_train</span><span class="p">,</span><span class="n">labels_train</span><span class="p">)</span>
<span class="n">labels_pred</span> <span class="o">=</span> <span class="n">clf_NB</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">features_test</span><span class="p">)</span>
<span class="k">print</span> <span class="n">classification_report</span><span class="p">(</span><span class="n">labels_test</span><span class="p">,</span> <span class="n">labels_pred</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="k">[&#39;poi&#39;, &#39;exercised_stock_options&#39;, &#39;total_stock_value&#39;, &#39;bonus&#39;, &#39;salary&#39;, &#39;fraction_to_poi&#39;, &#39;deferred_income&#39;]</span>
             <span class="err">precision</span>    <span class="err">recall</span>  <span class="err">f1-score</span>   <span class="err">support</span>

        <span class="err">0.0</span>       <span class="err">0.92</span>      <span class="err">0.94</span>      <span class="err">0.93</span>        <span class="err">36</span>
        <span class="err">1.0</span>       <span class="err">0.60</span>      <span class="err">0.50</span>      <span class="err">0.55</span>         <span class="err">6</span>

<span class="err">avg</span> <span class="err">/</span> <span class="err">total</span>       <span class="err">0.87</span>      <span class="err">0.88</span>      <span class="err">0.88</span>        <span class="err">42</span>
</pre></div>


<p>The precision and recall scores of my engineered list is higher than original list in POI detection (class 1). So I suggested to include 'fraction_to_poi' in feature set used for further investigation.</p>
<div class="highlight"><pre><span></span><span class="k">print</span> <span class="n">tabulate</span><span class="p">([[</span><span class="s1">&#39;Original List&#39;</span><span class="p">,</span> <span class="mf">0.33</span><span class="p">,</span> <span class="mf">0.33</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;Engineered list&#39;</span><span class="p">,</span> <span class="mf">0.60</span><span class="p">,</span> <span class="mf">0.50</span><span class="p">]],</span> <span class="n">headers</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Name&#39;</span><span class="p">,</span> <span class="s1">&#39;Precision&#39;</span><span class="p">,</span><span class="s1">&#39;Recall&#39;</span><span class="p">])</span>
</pre></div>


<div class="highlight"><pre><span></span>Name               Precision    Recall
---------------  -----------  --------
Original List           0.33      0.33
Engineered list         0.6       0.5
</pre></div>


<div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">featureFormat</span><span class="p">(</span><span class="n">my_dataset</span><span class="p">,</span> <span class="n">new_features_list</span><span class="p">,</span> <span class="n">sort_keys</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">labels</span><span class="p">,</span> <span class="n">features</span> <span class="o">=</span> <span class="n">targetFeatureSplit</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="c1"># Example starting point. Create train and test set from original dataset. 30% of data goes into test set.</span>
<span class="n">features_train</span><span class="p">,</span> <span class="n">features_test</span><span class="p">,</span> <span class="n">labels_train</span><span class="p">,</span> <span class="n">labels_test</span> <span class="o">=</span> \
    <span class="n">train_test_split</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="c1">#### Reshape the training and test sets.</span>
<span class="n">features_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">features_train</span><span class="p">)</span>
<span class="n">labels_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">labels_train</span><span class="p">)</span>
<span class="n">features_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">features_test</span><span class="p">)</span>
<span class="n">labels_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">labels_test</span><span class="p">)</span>
</pre></div>


<h2>3. Evaluation of classifiers</h2>
<p>There are many classifiers for classification. For the purpose of this porject and time saving, I only picked 6 common models for data fitting. I created pipeline that includes Scaling, Feature Selection, Principle Component Analysis (PCA) and Classifiers. I had to use feature scaling because some model need that processing.</p>
<p>Through running defaut model with 6 features of interest, their 'f1' scores were compared after running 'tester.py' . The reason for chosing 'f1' because it is the weighted average of Precision and Recall.</p>
<div class="highlight"><pre><span></span><span class="c1">### Try a varity of classifiers</span>
<span class="c1">### Please name your classifier clf for easy export below.</span>
<span class="c1">### Note that if you want to do PCA or other multi-stage operations,</span>
<span class="c1">### you&#39;ll need to use Pipelines. For more info:</span>
<span class="c1">### http://scikit-learn.org/stable/modules/pipeline.html</span>

<span class="c1">#### Checking the final result with all ML that are used for classification:</span>
<span class="c1">#### Using GridSearchCV to optimize number of features + SelectKBest</span>
<span class="c1">#### Create cross-validation  </span>
<span class="n">sss</span> <span class="o">=</span> <span class="n">StratifiedShuffleSplit</span><span class="p">(</span><span class="n">labels_train</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">)</span>
<span class="n">mm_scaler</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>

<span class="n">name_list</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Naive Bayes&#39;</span><span class="p">,</span>
             <span class="s1">&#39;Support Vector Machine&#39;</span><span class="p">,</span>
             <span class="s1">&#39;Decision Tree&#39;</span><span class="p">,</span>
             <span class="s1">&#39;KNN&#39;</span><span class="p">,</span>
             <span class="s1">&#39;RandomForest&#39;</span><span class="p">,</span><span class="s1">&#39;AdaBoost&#39;</span>
            <span class="p">]</span>
<span class="n">clf_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">GaussianNB</span><span class="p">(),</span>
            <span class="n">SVC</span><span class="p">(),</span>
            <span class="n">DecisionTreeClassifier</span><span class="p">(),</span>
            <span class="n">KNeighborsClassifier</span><span class="p">(),</span>
            <span class="n">RandomForestClassifier</span><span class="p">(),</span>
            <span class="n">AdaBoostClassifier</span><span class="p">()</span>
           <span class="p">]</span>

<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">clf</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">name_list</span><span class="p">,</span><span class="n">clf_list</span><span class="p">):</span>
    <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span> 
    <span class="n">pipe</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span> <span class="o">=</span><span class="p">[(</span><span class="s1">&#39;scaler&#39;</span><span class="p">,</span><span class="n">mm_scaler</span><span class="p">),(</span><span class="s1">&#39;SKB&#39;</span><span class="p">,</span><span class="n">SelectKBest</span><span class="p">(</span><span class="n">k</span> <span class="o">=</span> <span class="mi">6</span><span class="p">)),(</span><span class="s1">&#39;CLF&#39;</span><span class="p">,</span><span class="n">clf</span><span class="p">)])</span>
    <span class="n">pipe</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">features_train</span><span class="p">,</span><span class="n">labels_train</span><span class="p">)</span>
    <span class="n">labels_pred</span> <span class="o">=</span> <span class="n">pipe</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">features_test</span><span class="p">)</span>
    <span class="n">clf_final</span> <span class="o">=</span> <span class="n">pipe</span>
    <span class="c1">#clf_final = pipe</span>
    <span class="k">print</span> <span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span>
    <span class="k">print</span> <span class="n">name</span>
    <span class="k">print</span> <span class="s1">&#39;Fit to training dataset&#39;</span>
    <span class="k">print</span> <span class="n">classification_report</span><span class="p">(</span><span class="n">labels_test</span><span class="p">,</span> <span class="n">labels_pred</span><span class="p">)</span>    
    <span class="k">print</span> <span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span>
    <span class="k">print</span> <span class="s1">&#39;Test on tester.py&#39;</span>
    <span class="n">f1</span> <span class="o">=</span> <span class="n">tester</span><span class="o">.</span><span class="n">test_classifier</span><span class="p">(</span><span class="n">clf_final</span><span class="p">,</span> <span class="n">my_dataset</span><span class="p">,</span> <span class="n">new_features_list</span><span class="p">)</span>
    <span class="k">print</span> <span class="n">f1</span>
    <span class="k">print</span> <span class="s2">&quot;done in </span><span class="si">%0.3f</span><span class="s2">s&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>Naive Bayes
Fit to training dataset
             precision    recall  f1-score   support

        0.0       0.92      0.92      0.92        36
        1.0       0.50      0.50      0.50         6

avg / total       0.86      0.86      0.86        42


Test on tester.py
Pipeline(steps=[(&#39;scaler&#39;, MinMaxScaler(copy=True, feature_range=(0, 1))), (&#39;SKB&#39;, SelectKBest(k=6, score_func=&lt;function f_classif at 0x0000000007B63F98&gt;)), (&#39;CLF&#39;, GaussianNB())])
    Accuracy: 0.85236   Precision: 0.47723  Recall: 0.35100 F1: 0.40449 F2: 0.37061
    Total predictions: 14000    True positives:  702    False positives:  769   False negatives: 1298   True negatives: 11231

None
done in 2.235s

Support Vector Machine
Fit to training dataset
             precision    recall  f1-score   support

        0.0       0.86      1.00      0.92        36
        1.0       0.00      0.00      0.00         6

avg / total       0.73      0.86      0.79        42


Test on tester.py


C:\Anaconda2\lib\site-packages\sklearn\metrics\classification.py:1074: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  &#39;precision&#39;, &#39;predicted&#39;, average, warn_for)


Got a divide by zero when trying out: Pipeline(steps=[(&#39;scaler&#39;, MinMaxScaler(copy=True, feature_range=(0, 1))), (&#39;SKB&#39;, SelectKBest(k=6, score_func=&lt;function f_classif at 0x0000000007B63F98&gt;)), (&#39;CLF&#39;, SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape=None, degree=3, gamma=&#39;auto&#39;, kernel=&#39;rbf&#39;,
  max_iter=-1, probability=False, random_state=None, shrinking=True,
  tol=0.001, verbose=False))])
Precision or recall may be undefined due to a lack of true positive predicitons.
None
done in 2.101s

Decision Tree
Fit to training dataset
             precision    recall  f1-score   support

        0.0       0.91      0.89      0.90        36
        1.0       0.43      0.50      0.46         6

avg / total       0.84      0.83      0.84        42


Test on tester.py
Pipeline(steps=[(&#39;scaler&#39;, MinMaxScaler(copy=True, feature_range=(0, 1))), (&#39;SKB&#39;, SelectKBest(k=6, score_func=&lt;function f_classif at 0x0000000007B63F98&gt;)), (&#39;CLF&#39;, DecisionTreeClassifier(class_weight=None, criterion=&#39;gini&#39;, max_depth=None,
            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            presort=False, random_state=None, splitter=&#39;best&#39;))])
    Accuracy: 0.79071   Precision: 0.26912  Recall: 0.27100 F1: 0.27005 F2: 0.27062
    Total predictions: 14000    True positives:  542    False positives: 1472   False negatives: 1458   True negatives: 10528

None
done in 2.007s

KNN
Fit to training dataset
             precision    recall  f1-score   support

        0.0       0.85      0.94      0.89        36
        1.0       0.00      0.00      0.00         6

avg / total       0.73      0.81      0.77        42


Test on tester.py
Pipeline(steps=[(&#39;scaler&#39;, MinMaxScaler(copy=True, feature_range=(0, 1))), (&#39;SKB&#39;, SelectKBest(k=6, score_func=&lt;function f_classif at 0x0000000007B63F98&gt;)), (&#39;CLF&#39;, KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;,
           metric_params=None, n_jobs=1, n_neighbors=5, p=2,
           weights=&#39;uniform&#39;))])
    Accuracy: 0.83514   Precision: 0.23448  Recall: 0.06800 F1: 0.10543 F2: 0.07925
    Total predictions: 14000    True positives:  136    False positives:  444   False negatives: 1864   True negatives: 11556

None
done in 2.417s

RandomForest
Fit to training dataset
             precision    recall  f1-score   support

        0.0       0.89      0.92      0.90        36
        1.0       0.40      0.33      0.36         6

avg / total       0.82      0.83      0.83        42


Test on tester.py
Pipeline(steps=[(&#39;scaler&#39;, MinMaxScaler(copy=True, feature_range=(0, 1))), (&#39;SKB&#39;, SelectKBest(k=6, score_func=&lt;function f_classif at 0x0000000007B63F98&gt;)), (&#39;CLF&#39;, RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;,
            max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None...n_jobs=1,
            oob_score=False, random_state=None, verbose=0,
            warm_start=False))])
    Accuracy: 0.85629   Precision: 0.49239  Recall: 0.19400 F1: 0.27834 F2: 0.22076
    Total predictions: 14000    True positives:  388    False positives:  400   False negatives: 1612   True negatives: 11600

None
done in 27.533s

AdaBoost
Fit to training dataset
             precision    recall  f1-score   support

        0.0       0.92      0.92      0.92        36
        1.0       0.50      0.50      0.50         6

avg / total       0.86      0.86      0.86        42


Test on tester.py
Pipeline(steps=[(&#39;scaler&#39;, MinMaxScaler(copy=True, feature_range=(0, 1))), (&#39;SKB&#39;, SelectKBest(k=6, score_func=&lt;function f_classif at 0x0000000007B63F98&gt;)), (&#39;CLF&#39;, AdaBoostClassifier(algorithm=&#39;SAMME.R&#39;, base_estimator=None,
          learning_rate=1.0, n_estimators=50, random_state=None))])
    Accuracy: 0.82414   Precision: 0.35154  Recall: 0.27350 F1: 0.30765 F2: 0.28621
    Total predictions: 14000    True positives:  547    False positives: 1009   False negatives: 1453   True negatives: 10991

None
done in 118.370s
</pre></div>


<h3>Visualization of F1 scores</h3>
<div class="highlight"><pre><span></span><span class="c1">### Create array for f1</span>
<span class="n">f1_score</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.40449</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.27005</span><span class="p">,</span><span class="mf">0.10543</span><span class="p">,</span><span class="mf">0.27834</span><span class="p">,</span><span class="mf">0.30765</span><span class="p">]</span>

<span class="n">name_list</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;Naive Bayes&#39;</span><span class="p">,</span><span class="s1">&#39;Support Vector Machine&#39;</span><span class="p">,</span>
             <span class="s1">&#39;Decision Tree&#39;</span><span class="p">,</span><span class="s1">&#39;K-NN&#39;</span><span class="p">,</span>
             <span class="s1">&#39;RandomForest&#39;</span><span class="p">,</span><span class="s1">&#39;AdaBoost&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">name_list</span><span class="p">)),</span><span class="n">f1_score</span><span class="p">,</span><span class="n">align</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;rgbkymc&#39;</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">name_list</span><span class="p">)),</span> <span class="n">name_list</span><span class="p">,</span> <span class="n">rotation</span> <span class="o">=</span> <span class="mi">90</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;F1 score&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Classifier&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img alt="png" src="images/Enron_ML_36_0.png"></p>
<p>I found that Naive Bayes model had the highest F1-score. I decided to select Naive Bayes and Decision Tree for my model optimization. The final model is the one which highest F1 after parameter tuning.</p>
<h2>4. Model optimization</h2>
<p>The objective of this task is trying to optimize model fitting in order to obtain at least 0.3 in both precision and recall using testing script. </p>
<p>The strategy for tuning is using GridSearchCV to optimize parameters from each models and apply cross-validation to determine the best F1 score.</p>
<h3>4.1 Naive Bayes</h3>
<p>My strategy is to optimize:
- The number of features: SelectKBest(k = 5,6)
- The number of components: PCA(n_components = 2,3,4)</p>
<div class="highlight"><pre><span></span><span class="c1">### Create cross-validation</span>
<span class="n">sss</span> <span class="o">=</span> <span class="n">StratifiedShuffleSplit</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
<span class="c1"># PCA</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">()</span>
<span class="c1">#Pipeline</span>
<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([(</span><span class="s1">&#39;scale&#39;</span><span class="p">,</span><span class="n">mm_scaler</span><span class="p">),(</span><span class="s1">&#39;SKB&#39;</span><span class="p">,</span><span class="n">SelectKBest</span><span class="p">()),(</span><span class="s1">&#39;PCA&#39;</span><span class="p">,</span><span class="n">PCA</span><span class="p">()),(</span><span class="s1">&#39;NB&#39;</span><span class="p">,</span> <span class="n">GaussianNB</span><span class="p">())])</span>
<span class="c1"># clf&#39;s parameters</span>
<span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;SKB__k&#39;</span><span class="p">:[</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">],</span>
              <span class="s1">&#39;PCA__n_components&#39;</span><span class="p">:[</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span>
             <span class="p">}</span>
<span class="c1">#GridSearchCV</span>
<span class="n">gs</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">sss</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;f1&#39;</span><span class="p">)</span>
<span class="n">gs</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

<span class="n">clf_NB</span> <span class="o">=</span> <span class="n">gs</span><span class="o">.</span><span class="n">best_estimator_</span>
<span class="n">tester</span><span class="o">.</span><span class="n">test_classifier</span><span class="p">(</span><span class="n">clf_NB</span><span class="p">,</span> <span class="n">my_dataset</span><span class="p">,</span> <span class="n">new_features_list</span><span class="p">)</span>
<span class="k">print</span> <span class="s2">&quot;done in </span><span class="si">%0.3f</span><span class="s2">s&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>C:\Anaconda2\lib\site-packages\sklearn\metrics\classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  &#39;precision&#39;, &#39;predicted&#39;, average, warn_for)


Pipeline(steps=[(&#39;scale&#39;, MinMaxScaler(copy=True, feature_range=(0, 1))), (&#39;SKB&#39;, SelectKBest(k=6, score_func=&lt;function f_classif at 0x0000000007B63F98&gt;)), (&#39;PCA&#39;, PCA(copy=True, n_components=4, whiten=False)), (&#39;NB&#39;, GaussianNB())])
    Accuracy: 0.85564   Precision: 0.49256  Recall: 0.34750 F1: 0.40751 F2: 0.36925
    Total predictions: 14000    True positives:  695    False positives:  716   False negatives: 1305   True negatives: 11284

done in 5.306s
</pre></div>


<h3>4.2 Decision Tree</h3>
<p>In this classifier, there are many parameters that I could tune. Because of heavy computation, I selected ony 5 parameters: criterion, splitter, min_samples_split, max_depth, max_leaf_nodes. Those parameters will be put in the cross-validation with feature selection and component analysis.</p>
<div class="highlight"><pre><span></span><span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
<span class="c1"># PCA</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">()</span>
<span class="c1">#Pipeline</span>
<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([(</span><span class="s1">&#39;scale&#39;</span><span class="p">,</span><span class="n">mm_scaler</span><span class="p">),(</span><span class="s1">&#39;SKB&#39;</span><span class="p">,</span><span class="n">SelectKBest</span><span class="p">()),(</span><span class="s1">&#39;PCA&#39;</span><span class="p">,</span><span class="n">PCA</span><span class="p">()),(</span><span class="s1">&#39;DT&#39;</span><span class="p">,</span> <span class="n">DecisionTreeClassifier</span><span class="p">())])</span>
<span class="c1"># clf&#39;s parameters</span>
<span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;DT__criterion&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;gini&#39;</span><span class="p">,</span><span class="s1">&#39;entropy&#39;</span><span class="p">],</span>
              <span class="s1">&#39;DT__splitter&#39;</span><span class="p">:[</span><span class="s1">&#39;best&#39;</span><span class="p">,</span><span class="s1">&#39;random&#39;</span><span class="p">],</span>
              <span class="s1">&#39;DT__min_samples_split&#39;</span><span class="p">:[</span><span class="mi">2</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">20</span><span class="p">],</span>
              <span class="s1">&#39;DT__max_depth&#39;</span><span class="p">:[</span><span class="mi">10</span><span class="p">,</span><span class="mi">15</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">25</span><span class="p">,</span><span class="mi">30</span><span class="p">],</span>
              <span class="s1">&#39;DT__max_leaf_nodes&#39;</span><span class="p">:[</span><span class="mi">5</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">30</span><span class="p">],</span>
              <span class="s1">&#39;SKB__k&#39;</span><span class="p">:[</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">],</span>
              <span class="s1">&#39;PCA__n_components&#39;</span><span class="p">:[</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">]}</span>
<span class="c1">#GridSearchCV</span>
<span class="n">gs</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">sss</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;f1&#39;</span><span class="p">)</span>
<span class="n">gs</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

<span class="n">clf_DT</span> <span class="o">=</span> <span class="n">gs</span><span class="o">.</span><span class="n">best_estimator_</span>
<span class="n">tester</span><span class="o">.</span><span class="n">test_classifier</span><span class="p">(</span><span class="n">clf_DT</span><span class="p">,</span> <span class="n">my_dataset</span><span class="p">,</span> <span class="n">new_features_list</span><span class="p">)</span>
<span class="k">print</span> <span class="s2">&quot;done in </span><span class="si">%0.3f</span><span class="s2">s&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>Pipeline(steps=[(&#39;scale&#39;, MinMaxScaler(copy=True, feature_range=(0, 1))), (&#39;SKB&#39;, SelectKBest(k=5, score_func=&lt;function f_classif at 0x0000000007B63F98&gt;)), (&#39;PCA&#39;, PCA(copy=True, n_components=2, whiten=False)), (&#39;DT&#39;, DecisionTreeClassifier(class_weight=None, criterion=&#39;entropy&#39;, max_depth=25,
            max_features=None, max_leaf_nodes=30, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            presort=False, random_state=None, splitter=&#39;best&#39;))])
    Accuracy: 0.82007   Precision: 0.35639  Recall: 0.32200 F1: 0.33832 F2: 0.32834
    Total predictions: 14000    True positives:  644    False positives: 1163   False negatives: 1356   True negatives: 10837

done in 669.987s
</pre></div>


<h3>4.3 Discussion</h3>
<p>In this section, I tried to to make an machine system that can built the best model from data. In machine system, chosen learning algorithm goes with multiple parameters. All those parameters will effect the quality of data models. However, it is not easy to find the correct values of those parameters. Thus, tuning is one of the most important in machine learning. This step will help us to obtain optimal values to complete learning task in the best way as possible. The "best" of those values is dependent on our criteria. For instance, I mainly tuned parameters of Naive Bayes and Decision Tree models to obtain at least 0.3 in both precision and recall in this project. </p>
<p>In two final results, Naive Bayes provide better score of precision and recall. It is logical since the Enron dataset is pretty small so simpler model will be easily fit. In conclusion, Naive Bayes model is my final model for this Enron fraud classification.</p>
<div class="highlight"><pre><span></span><span class="n">clf</span> <span class="o">=</span> <span class="n">clf_NB</span>
<span class="n">dump_classifier_and_data</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">my_dataset</span><span class="p">,</span> <span class="n">new_features_list</span><span class="p">)</span>
</pre></div>


<h2>5. Validation and Evaluation</h2>
<p>The main purpose of this project is to identify the best learning model for Enron dataset. From previous analysis, there are more than one approach to build that model. In learning approach, one of process is parameter tuning. The "best" choice of parameters requires validation. Validation estimates how well my model has been trained. Validation includes splitting Enron data into multiple train/test sets. This step is essential because it limits problem such as data overfitting or underfitting. In this project, I have used "train_test_split" and "StratifiedShuffleSplit" for re-sample Enron data. "TrainTestSplit" created small dataset to choose the appropriate classifiers. "StratifiedShuffleSplit" was used for cross-validation of optimal parameters.</p>
<p>Instead of using simpler cross-validation methods, I picked "StratifiedShuffleSplit" for my final learning algorithm. Enron dataset is quite small so "StratifiedShuffleSplit" allows to create randomly multiple training and test sets then averages the results over all the tests. Moreover, Enrone dataset has unbalanced classes where number of POIs is greatly smaller than number of non-POIs. "StratifiedShuffleSplit" makes sure the ratio of POI/non-POIs is the same in the training and test sets as it was in the larger dataset. </p>
<p>I used precision and recall to estimate my model properties. The precision measures a classifiers exactness and the recall measures a classifiers completeness. In small and unbalanced like Enron dataset, both of them are more important than accuracy. If validation is not good, my model will fall in high level of error type I and II. We always want to have high chance to catch criminal but ignore innocent person.</p>
<p>The good estimators for my Enron classification model is the one which has the score of precision and recall above 0.3. </p>
<h2>6. Conclusion</h2>
<p>This Enron dataset is an unbalaned data where POIs is greatly samller than non-POIs. There are 3 outliers and many of missing values in each features of data point. I have removed all of outliers and replaced missing values by '0' during data auditing.</p>
<p>On the other hands, I optimized Feature Selection by creating two new features: "fraction_to_poi" and "fraction_from_poi". According to the impact scores, I have selected 6 features to build my learning model. One of them was my new engineered feature. Without it, I have lower score in term of precision and recall. </p>
<p>For learning model, I validated a series of common classifers. Based on my F1-score, I picked two classifiers: Naive Bayes and Decisition Tree. My final model is Naive Bayes model in which I receive the better score of precision (0.49) and recall (0.34) after parameter tuning.</p>                </article>
            </aside><!-- /#featured -->
        <section id="extras" class="body">
                <div class="social">
                        <h2>social</h2>
                        <ul>
                            <li><a href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate">atom feed</a></li>

                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="http://getpelican.com/">Pelican</a>, which takes great advantage of <a href="http://python.org">Python</a>.
                </address><!-- /#about -->

                <p>The theme is by <a href="http://coding.smashingmagazine.com/2009/08/04/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
        </footer><!-- /#contentinfo -->

</body>
</html>